{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# FYP Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import catboost as ctb\n",
    "# !pip install imbalanced-learn\n",
    "import imblearn\n",
    "# !pip install catboost\n",
    "# !pip install xgboost\n",
    "import xgboost as XGB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.metrics import average_precision_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Exploring the Dataset in Python\n",
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading fyp dataset\n",
    "df=pd.read_csv(\"SafeNav Data Collection Survey (Responses) 500.csv\")\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()    #printing info about df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)  #first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Step1: Checking for duplicate rows (Data Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicate values\n",
    "duplicate_values = df.duplicated()\n",
    "print(duplicate_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing duplicate rows, considering first duplicate row as unique\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "df=df.reset_index(drop=True)   #resetting index\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "==> Established that there are no duplicate rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Step2: Checking for null values (Data Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing=df.isna().sum()   #counting null values in dataset\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "==> Established that there are no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Step3: Removing irrelevant attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "==> We will remove Timestamp, Email Address and Date of Incident columns since it doesn't really seem to affect the way in     which a machine would learn data, atleast to the naked eye.\n",
    "\n",
    "==> Age, Gender, District, Crime, Transportation mode used features will also be removed, but after Data Visualisation so    we can study the underlying phenomenon that is generating the data.\n",
    "\n",
    "==> Nearby location of incident feature will be removed after evaluating target variable.\n",
    "\n",
    "==> We will have 3 input/feature variables namely Time of Incident, Latitude and Longitude.\n",
    "\n",
    "==> Output/Target variable will be Crime Score which will be evaluated later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping irrelevant columns from dataset\n",
    "df.drop([\"Timestamp\"], axis=1, inplace=True)\n",
    "df.drop([\"Email Address\"], axis=1, inplace=True)\n",
    "df.drop([\"Date of Incident\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reset_index(drop=True)   #resetting index to default integer index\n",
    "df   #changes incorporated permanently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Step4: Data Visualization ==> Checking and removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "==> Outliers can only be checked for numerical data, our dataset contains only two numerical (float) features namely Latitude and Longitude. However, we will not check these features for outliers as their data was manually prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.boxplot(column =['Latitude','Longitude'],figsize=(10,10), grid = False)   #constructing boxplot\n",
    "# plt.title('Box Plot Of Features Before Removing Outliers')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For Latitude feature\n",
    "# # Calculate the interquartile range (IQR)\n",
    "# Q1 = df['Latitude'].quantile(0.25)\n",
    "# Q3 = df['Latitude'].quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "\n",
    "# # Define the threshold for Latitude outliers\n",
    "# threshold = 1.5\n",
    "\n",
    "# # Remove Latitude outliers from the dataset\n",
    "# df= df[(df['Latitude'] >= Q1 - threshold * IQR) &\n",
    "#                              (df['Latitude'] <= Q3 + threshold * IQR)]\n",
    "\n",
    "# #For Longitude feature\n",
    "# # Calculate the interquartile range (IQR)\n",
    "# Q1 = df['Longitude'].quantile(0.25)\n",
    "# Q3 = df['Longitude'].quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "\n",
    "# # Define the threshold for Longitude outliers\n",
    "# threshold = 1.5\n",
    "\n",
    "# # Remove Longitude outliers from the dataset\n",
    "# df= df[(df['Longitude'] >= Q1 - threshold * IQR) &\n",
    "#                              (df['Longitude'] <= Q3 + threshold * IQR)]\n",
    "\n",
    "# df=df.reset_index(drop=True)   #resetting index\n",
    "# df   #after removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.boxplot(column =['Latitude','Longitude'],figsize=(10,10), grid = False)   #constructing boxplot\n",
    "# plt.title('Box Plot Of Features After Removing Outliers')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Making Histograms, ScatterPlots and CountPlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_style(\"whitegrid\")\n",
    "# plt.figure(figsize=(6,6))\n",
    "# plt.pie(df['Age'], labels=df['Age'], autopct='%1.1f%%')\n",
    "# plt.title('My Pie Chart')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking number of instances of each unique value in District feature\n",
    "df[\"District\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting distribution of the input variables\n",
    "plt.figure(figsize=(18,7))\n",
    "sns.countplot(x=\"District\", data = df)               #countplot\n",
    "plt.title('Count Of No Of Samples Of District Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting distribution of the input variable\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=\"Crime\", data = df)               #countplot\n",
    "plt.title('Count Of No Of Samples Of Crime Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting distribution of the input variable\n",
    "plt.figure(figsize=(18, 7))\n",
    "sns.countplot(x ='District', hue = 'Crime', data = df)             #countplot\n",
    "plt.title('DISTRICT vs CRIME Data Distribition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting distribution of the input variable\n",
    "plt.figure(figsize=(18, 5))\n",
    "sns.countplot(x ='District', hue = 'Time of Incident', data = df)             #countplot\n",
    "plt.title('DISTRICT vs TIME OF INCIDENT Data Distribition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting distribution of the input variable\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x ='District', hue = 'Transportation mode used', data = df)             #countplot\n",
    "plt.title('DISTRICT vs TRANSPORTATION MODE USED Data Distribition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting distribution of the input variable\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x ='Crime', hue = 'Transportation mode used', data = df)             #countplot\n",
    "plt.title('CRIME vs TRANSPORTATION MODE USED Data Distribition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting distribution of the input variable\n",
    "# plt.figure(figsize=(10, 5))\n",
    "sns.countplot(y ='Gender', hue = 'Age', data = df)               #countplot\n",
    "plt.title('GENDER vs AGE Data Distribition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting distribution of the input variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(y ='Age', hue = 'Crime', data = df)             #countplot\n",
    "plt.title('AGE vs CRIME Data Distribition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting distribution of the input variable\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(y ='Gender', hue = 'Crime', data = df)             #countplot\n",
    "plt.title('GENDER vs CRIME Data Distribition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysing data\n",
    "df.hist(column='Latitude')   #histogram\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysing data\n",
    "df.hist(column='Longitude')   #histogram\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysing data\n",
    "plt.scatter(df[\"Latitude\"], df[\"Longitude\"])    #scatterplot\n",
    "plt.title('Scatter Plot')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "==> Very low correlation can be observed between Latitude and Longitude variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "###### Now, we will drop the remaining irrelevant columns except for Nearby location of incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping irrelevant columns from dataset\n",
    "df.drop([\"Age\"], axis=1, inplace=True)\n",
    "df.drop([\"Gender\"], axis=1, inplace=True)\n",
    "df.drop([\"District\"], axis=1, inplace=True)\n",
    "df.drop([\"Crime\"], axis=1, inplace=True)\n",
    "df.drop([\"Transportation mode used\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reset_index(drop=True)   #resetting index to default integer index\n",
    "df   #changes incorporated permanently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Step5: Adding and Evaluating the Target Variable \"Crime_Score\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "==> Crime score will be calculated by counting the number of occurreneces of the longitude & latitude of a location for a particular Time of Incident.\n",
    "\n",
    "==> Time of Incident is divided into four time zones as seen during data visualization.\n",
    "\n",
    "==> Once Crime Score is evaluated, we will remove the duplicate rows keeping only the last instance of each duplicate row which will then give us the Crime Score for each unique location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking number of unique locations in dataset\n",
    "df[\"Latitude\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Crime Score Column\n",
    "value = 0    #initially setting to 0\n",
    " \n",
    "# Add the new column using loc\n",
    "df.loc[:, \"Crime_Score\"] = value\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "==> We have used the GroupBy.cumcount() function to evaluate Crime Score. This function returns the cumulative count of occurrences within each group. The “groupby” operation in pandas is used to split a DataFrame into groups based on some criteria. It creates a GroupBy object that can be used to perform various operations on each group. The “cumcount” method is applied to a GroupBy object which in this case includes Time of Incident, Latitude and Longitude columns, and then computes the cumulative count of occurrences within each group. It starts from 0 and increments by 1 for each occurrence in the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using cumcount() from groupby. It counts the amount of the same values in each group starting from 0.\n",
    "s = df.groupby(['Time of Incident', 'Latitude', 'Longitude']).cumcount()\n",
    "df['Crime_Score'] =  s+1\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "==> Now, we will remove the duplicate rows keeping only the last instance of each duplicate row which will then give us the     Crime Score for each unique location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing duplicate rows, considering first duplicate row as unique\n",
    "df.drop_duplicates(subset=['Time of Incident','Latitude','Longitude'], keep='last', inplace=True)\n",
    "df=df.reset_index(drop=True)   #resetting index\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display entire dataset\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "###### removing Nearby location of incident feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"Nearby location of incident\"], axis=1, inplace=True)\n",
    "df=df.reset_index(drop=True)   #resetting index to default integer index\n",
    "df   #changes incorporated permanently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Step6: Checking for categorical variables and encoding them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "###### Categorical variables can be of three types namely binary, nominal and ordinal. It's quite visible that our data consist of only one categorical variable i.e Time of Incident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking datatype of every column\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "###### Time of Incident feature contains ordinal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking number of instances of each unique value in Time of Incident feature\n",
    "df[\"Time of Incident\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Label Encoding for ordinal variable\n",
    "le = preprocessing.LabelEncoder()\n",
    "df[\"Time of Incident\"] = le.fit_transform(df[\"Time of Incident\"])\n",
    "df[\"Time of Incident\"].unique()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking number of instances of each unique value in Time of Incident feature after encoding\n",
    "df[\"Time of Incident\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking number of instances of each unique value in Target variable\n",
    "df[\"Crime_Score\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "###### It can be observed that our dataset is imbalanced. To resolve the issue , we will use a data augmentation technique \"SMOTE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Step7: Checking for multicollinearity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking/reviewing correlation matrices\n",
    "corrM = df.corr()\n",
    "corrM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Coefficient – Pearson’s Correlation Coefficient \n",
    "corr = df.corr()\n",
    "\n",
    "#Plotting Heatmap\n",
    "plt.figure(figsize = (10,6))\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "###### As observed, none of the features are strongly positively/negatively correlated, hence there isn't any need of removing columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "### Splitting data into Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing data into input and output\n",
    "inp=df.iloc[:,0:3]\n",
    "out=df.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying SMOTE oversampling technique\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42, k_neighbors=1)\n",
    "X_res, y_res = sm.fit_resample(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking number of instances of each unique value in Target variable after oversampling\n",
    "y_res.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "###### Our dataset is now balanced. The minority classes have the same number of samples as the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.2, random_state=0)\n",
    "print('Training set shape: ', x_train.shape, y_train.shape)\n",
    "print('Testing set shape: ', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "##### Feature scaling is not implemented as it doesn't seem to have much impact on the performance of ML algorithms used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Data Scaling\n",
    "# #Applying standardization \n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# #Train Set -> Fit_transform\n",
    "# x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "# #Test Set -> Transform\n",
    "# x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "# **Implementing Machine learning Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_precisions=[]    #creating empty list which will store weighted_precisions scores of all models\n",
    "#Training and Testing Model 1\n",
    "#Importing RandomForest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Creating object and fitting data onto the model\n",
    "rf_1=RandomForestClassifier(n_estimators=100, max_depth=80, min_samples_split=8).fit(x_train,y_train)\n",
    "y_tr=rf_1.predict(x_train)\n",
    "y_pred = rf_1.predict(x_test)\n",
    "acc=metrics.accuracy_score(y_test, y_pred)\n",
    "wprec=metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "wrecall=metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "wf1=metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Train Accuracy is \"  + str (metrics.accuracy_score(y_train, y_tr)*100) + \"%\")\n",
    "print(\"Test Accuracy is \"  + str (acc*100) + \"%\")\n",
    "print(\"Weighted Precision Score is \"  + str (wprec*100) + \"%\")\n",
    "print(\"Weighted Recall Score is \"  + str (wrecall*100) + \"%\")\n",
    "print(\"Weighted F1 Score is \"  + str (wf1*100) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Confusion Matrix \"+\"\\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "# target_names = [\"Safe\", \"Unsafe\"]\n",
    "print(\"Classification Report\"+\"\\n\",classification_report(y_test, y_pred))\n",
    "weighted_precisions.append([\"Random Forest Model 1\", wprec, acc, wrecall, wf1])   #appending models score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing Model 2\n",
    "#Importing RandomForest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Creating object and fitting data onto the model\n",
    "rf_2=RandomForestClassifier(n_estimators=150, max_depth=80, min_samples_split=4).fit(x_train,y_train)\n",
    "y_tr=rf_2.predict(x_train)\n",
    "y_pred = rf_2.predict(x_test)\n",
    "acc=metrics.accuracy_score(y_test, y_pred)\n",
    "wprec=metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "wrecall=metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "wf1=metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Train Accuracy is \"  + str (metrics.accuracy_score(y_train, y_tr)*100) + \"%\")\n",
    "print(\"Test Accuracy is \"  + str (acc*100) + \"%\")\n",
    "print(\"Weighted Precision Score is \"  + str (wprec*100) + \"%\")\n",
    "print(\"Weighted Recall Score is \"  + str (wrecall*100) + \"%\")\n",
    "print(\"Weighted F1 Score is \"  + str (wf1*100) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Confusion Matrix \"+\"\\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "# target_names = [\"Safe\", \"Unsafe\"]\n",
    "print(\"Classification Report\"+\"\\n\",classification_report(y_test, y_pred))\n",
    "weighted_precisions.append([\"Random Forest Model 2\", wprec, acc, wrecall, wf1])   #appending models score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing Model 3\n",
    "#Importing RandomForest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Creating object and fitting data onto the model\n",
    "rf_3=RandomForestClassifier(n_estimators=200, max_depth=100, min_samples_split=3).fit(x_train,y_train)\n",
    "y_tr=rf_3.predict(x_train)\n",
    "y_pred = rf_3.predict(x_test)\n",
    "acc=metrics.accuracy_score(y_test, y_pred)\n",
    "wprec=metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "wrecall=metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "wf1=metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Train Accuracy is \"  + str (metrics.accuracy_score(y_train, y_tr)*100) + \"%\")\n",
    "print(\"Test Accuracy is \"  + str (acc*100) + \"%\")\n",
    "print(\"Weighted Precision Score is \"  + str (wprec*100) + \"%\")\n",
    "print(\"Weighted Recall Score is \"  + str (wrecall*100) + \"%\")\n",
    "print(\"Weighted F1 Score is \"  + str (wf1*100) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Confusion Matrix \"+\"\\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "# target_names = [\"Safe\", \"Unsafe\"]\n",
    "print(\"Classification Report\"+\"\\n\",classification_report(y_test, y_pred))\n",
    "weighted_precisions.append([\"Random Forest Model 3\", wprec, acc, wrecall, wf1])   #appending models score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing Model 4\n",
    "#Importing RandomForest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Creating object and fitting data onto the model\n",
    "rf_4=RandomForestClassifier(n_estimators=250, max_depth=120, min_samples_split=4).fit(x_train,y_train)\n",
    "y_tr=rf_4.predict(x_train)\n",
    "y_pred = rf_4.predict(x_test)\n",
    "acc=metrics.accuracy_score(y_test, y_pred)\n",
    "wprec=metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "wrecall=metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "wf1=metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Train Accuracy is \"  + str (metrics.accuracy_score(y_train, y_tr)*100) + \"%\")\n",
    "print(\"Test Accuracy is \"  + str (acc*100) + \"%\")\n",
    "print(\"Weighted Precision Score is \"  + str (wprec*100) + \"%\")\n",
    "print(\"Weighted Recall Score is \"  + str (wrecall*100) + \"%\")\n",
    "print(\"Weighted F1 Score is \"  + str (wf1*100) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Confusion Matrix \"+\"\\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "# target_names = [\"Safe\", \"Unsafe\"]\n",
    "print(\"Classification Report\"+\"\\n\",classification_report(y_test, y_pred))\n",
    "weighted_precisions.append([\"Random Forest Model 4\", wprec, acc, wrecall, wf1])   #appending models score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# # Defining our possible hyperparameters\n",
    "# grid_hyperparameters_rf = {'n_estimators': [100,250,500,1000], 'max_depth': [50,100,250,500], \n",
    "#                          'min_samples_split': [2,3,4,5]}\n",
    "# # Searching for best hyperparameters\n",
    "# grid_rf = GridSearchCV(estimator=RandomForestClassifier(), param_grid=grid_hyperparameters_rf, cv=3, scoring='accuracy')\n",
    "# grid_rf.fit(inp, out)\n",
    "# # Getting the results\n",
    "# print(\"Best Score is \",grid_rf.best_score_)\n",
    "# print(\"Best Estimator is \",grid_rf.best_estimator_)\n",
    "# print(\"Best Parameter combination is \",grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "|^| In our example above we have 64 unique combinations of hyperparameters (4 hyperparameter values for n_estimators times 4 hyperparameter values for max_depth times 4 hyperparameterp values for min_samples_split). For each of these 64 combinations, the 3-fold cross-validation (cv=3) creates 3 models. So in this example, GridSearchCV() creates and evaluates 192 (64x3) models and then then determines the best model out of these 192 and prints the corresponding accuracy and hyperparameters used for that model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "### CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing Model 1\n",
    "#Importing CatBoost Classifier\n",
    "from catboost import CatBoostClassifier\n",
    "#Creating object and fitting data onto the model\n",
    "cat_1 = CatBoostClassifier(iterations=250, learning_rate=0.1).fit(x_train, y_train)\n",
    "y_tr=cat_1.predict(x_train)\n",
    "y_pred = cat_1.predict(x_test)\n",
    "acc=metrics.accuracy_score(y_test, y_pred)\n",
    "wprec=metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "wrecall=metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "wf1=metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"\\n\")\n",
    "print(\"Train Accuracy is \"  + str (metrics.accuracy_score(y_train, y_tr)*100) + \"%\")\n",
    "print(\"Test Accuracy is \"  + str (acc*100) + \"%\")\n",
    "print(\"Weighted Precision Score is \"  + str (wprec*100) + \"%\")\n",
    "print(\"Weighted Recall Score is \"  + str (wrecall*100) + \"%\")\n",
    "print(\"Weighted F1 Score is \"  + str (wf1*100) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Confusion Matrix \"+\"\\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "# target_names = [\"Safe\", \"Unsafe\"]\n",
    "print(\"Classification Report\"+\"\\n\",classification_report(y_test, y_pred))\n",
    "weighted_precisions.append([\"CatBoost Model 1\", wprec, acc, wrecall, wf1])   #appending models score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing Model 2\n",
    "#Importing CatBoost Classifier\n",
    "from catboost import CatBoostClassifier\n",
    "#Creating object and fitting data onto the model\n",
    "cat_2 = CatBoostClassifier(iterations=250, learning_rate=0.2).fit(x_train, y_train)\n",
    "y_tr=cat_2.predict(x_train)\n",
    "y_pred = cat_2.predict(x_test)\n",
    "acc=metrics.accuracy_score(y_test, y_pred)\n",
    "wprec=metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "wrecall=metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "wf1=metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"\\n\")\n",
    "print(\"Train Accuracy is \"  + str (metrics.accuracy_score(y_train, y_tr)*100) + \"%\")\n",
    "print(\"Test Accuracy is \"  + str (acc*100) + \"%\")\n",
    "print(\"Weighted Precision Score is \"  + str (wprec*100) + \"%\")\n",
    "print(\"Weighted Recall Score is \"  + str (wrecall*100) + \"%\")\n",
    "print(\"Weighted F1 Score is \"  + str (wf1*100) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Confusion Matrix \"+\"\\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "# target_names = [\"Safe\", \"Unsafe\"]\n",
    "print(\"Classification Report\"+\"\\n\",classification_report(y_test, y_pred))\n",
    "weighted_precisions.append([\"CatBoost Model 2\", wprec, acc, wrecall, wf1])   #appending models score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing Model 3\n",
    "#Importing CatBoost Classifier\n",
    "from catboost import CatBoostClassifier\n",
    "#Creating object and fitting data onto the model\n",
    "cat_3 = CatBoostClassifier(iterations=500, learning_rate=0.1).fit(x_train, y_train)\n",
    "y_tr=cat_3.predict(x_train)\n",
    "y_pred = cat_3.predict(x_test)\n",
    "acc=metrics.accuracy_score(y_test, y_pred)\n",
    "wprec=metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "wrecall=metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "wf1=metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"\\n\")\n",
    "print(\"Train Accuracy is \"  + str (metrics.accuracy_score(y_train, y_tr)*100) + \"%\")\n",
    "print(\"Test Accuracy is \"  + str (acc*100) + \"%\")\n",
    "# print(\"Weighted Precision Score is \"  + str (wprec*100) + \"%\")\n",
    "# print(\"Weighted Recall Score is \"  + str (wrecall*100) + \"%\")\n",
    "# print(\"Weighted F1 Score is \"  + str (wf1*100) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Confusion Matrix \"+\"\\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification Report\"+\"\\n\",classification_report(y_test, y_pred))\n",
    "weighted_precisions.append([\"CatBoost Model 3\", wprec, acc, wrecall, wf1])   #appending models score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing Model 4\n",
    "#Importing CatBoost Classifier\n",
    "from catboost import CatBoostClassifier\n",
    "#Creating object and fitting data onto the model\n",
    "cat_4 = CatBoostClassifier(iterations=750, learning_rate=0.3).fit(x_train, y_train)\n",
    "y_tr=cat_4.predict(x_train)\n",
    "y_pred = cat_4.predict(x_test)\n",
    "acc=metrics.accuracy_score(y_test, y_pred)\n",
    "wprec=metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "wrecall=metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "wf1=metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"\\n\")\n",
    "print(\"Train Accuracy is \"  + str (metrics.accuracy_score(y_train, y_tr)*100) + \"%\")\n",
    "print(\"Test Accuracy is \"  + str (acc*100) + \"%\")\n",
    "print(\"Weighted Precision Score is \"  + str (wprec*100) + \"%\")\n",
    "print(\"Weighted Recall Score is \"  + str (wrecall*100) + \"%\")\n",
    "print(\"Weighted F1 Score is \"  + str (wf1*100) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Confusion Matrix \"+\"\\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification Report\"+\"\\n\",classification_report(y_test, y_pred))\n",
    "weighted_precisions.append([\"CatBoost Model 4\", wprec, acc, wrecall, wf1])   #appending models score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# # Defining our possible hyperparameters\n",
    "# grid_hyperparameters_cat = {'iterations': [250,500,750,1000], 'learning_rate': [0.1,0.4,0.7,1.0], \n",
    "#                          'depth': [2,4,6,8]}\n",
    "# # Searching for best hyperparameters\n",
    "# grid_cat = GridSearchCV(estimator=CatBoostClassifier(), param_grid=grid_hyperparameters_cat, cv=3, scoring='accuracy')\n",
    "# grid_cat.fit(inp, out)\n",
    "# # Getting the results\n",
    "# print(\"\\n\")\n",
    "# print(\"Best Score is \",grid_cat.best_score_)\n",
    "# print(\"Best Estimator is \",grid_cat.best_estimator_)\n",
    "# print(\"Best Parameter combination is \",grid_cat.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "|^| In our example above we have 64 unique combinations of hyperparameters (4 hyperparameter values for iterations times 4 hyperparameter values for learning_rate times 4 hyperparameterp values for depth). For each of these 64 combinations, the 3-fold cross-validation (cv=3) creates 3 models. So in this example, GridSearchCV() creates and evaluates 192 (64x3) models and then then determines the best model out of these 192 and prints the corresponding accuracy and hyperparameters used for that model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "### KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing Model 1\n",
    "#Importing KNeighbors Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Creating object and fitting data onto the model\n",
    "knn_1=KNeighborsClassifier(n_neighbors=3,metric='minkowski',weights='uniform').fit(x_train,y_train);\n",
    "y_tr=knn_1.predict(x_train)\n",
    "y_pred=knn_1.predict(x_test)\n",
    "# print(y_pred)\n",
    "print(\"Train Accuracy is \"  + str (metrics.accuracy_score(y_train, y_tr)*100) + \"%\")\n",
    "acc=metrics.accuracy_score(y_test, y_pred)\n",
    "wprec=metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "wrecall=metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "wf1=metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Test Accuracy is \"  + str (acc*100) + \"%\")\n",
    "print(\"Weighted Precision Score is \"  + str (wprec*100) + \"%\")\n",
    "print(\"Weighted Recall Score is \"  + str (wrecall*100) + \"%\")\n",
    "print(\"Weighted F1 Score is \"  + str (wf1*100) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Confusion Matrix \"+\"\\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification Report\"+\"\\n\",classification_report(y_test, y_pred))\n",
    "# weighted_precisions.append([\"KNeighbors Classifier Model 1\", wprec, acc, wrecall, wf1])   #appending models score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing Model 1\n",
    "#Importing Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "#Creating object and fitting data onto the model\n",
    "logreg_1 = LogisticRegression(solver='sag', multi_class='ovr').fit(x_train , y_train)\n",
    "y_tr=logreg_1.predict(x_train)\n",
    "y_pred=logreg_1.predict(x_test)\n",
    "print(\"Train Accuracy is \"  + str (metrics.accuracy_score(y_train, y_tr)*100) + \"%\")\n",
    "acc=metrics.accuracy_score(y_test, y_pred)\n",
    "wprec=metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "wrecall=metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "wf1=metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Test Accuracy is \"  + str (acc*100) + \"%\")\n",
    "print(\"Weighted Precision Score is \"  + str (wprec*100) + \"%\")\n",
    "print(\"Weighted Recall Score is \"  + str (wrecall*100) + \"%\")\n",
    "print(\"Weighted F1 Score is \"  + str (wf1*100) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Confusion Matrix \"+\"\\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification Report\"+\"\\n\",classification_report(y_test, y_pred))\n",
    "# weighted_precisions.append([\"Logistic Regression Model 1\", wprec, acc, wrecall, wf1])   #appending models score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing Model 1\n",
    "# Importing MLPClassifer \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Create model object and fitting data onto the model\n",
    "mlp_1 = MLPClassifier(hidden_layer_sizes=(80,80), activation='relu', solver='sgd',\n",
    "                      learning_rate='adaptive', early_stopping=True).fit(x_train,y_train)\n",
    "y_tr=mlp_1.predict(x_train)\n",
    "y_pred=mlp_1.predict(x_test)\n",
    "print(\"Train Accuracy is \"  + str (metrics.accuracy_score(y_train, y_tr)*100) + \"%\")\n",
    "acc=metrics.accuracy_score(y_test, y_pred)\n",
    "wprec=metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "wrecall=metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "wf1=metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Test Accuracy is \"  + str (acc*100) + \"%\")\n",
    "print(\"Weighted Precision Score is \"  + str (wprec*100) + \"%\")\n",
    "print(\"Weighted Recall Score is \"  + str (wrecall*100) + \"%\")\n",
    "print(\"Weighted F1 Score is \"  + str (wf1*100) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Confusion Matrix \"+\"\\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification Report\"+\"\\n\",classification_report(y_test, y_pred))\n",
    "# weighted_precisions.append([\"MultiLayer Perceptron Model 1\", wprec, acc, wrecall, wf1])   #appending models score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# we can add class_weight='balanced' to add panalize mistake\n",
    "svc_model = SVC(class_weight='balanced',probability=True).fit(x_train, y_train)\n",
    "y_tr=svc_model.predict(x_train)\n",
    "svc_pred = svc_model.predict(x_test)# check performance\n",
    "print(\"Train Accuracy is \"  + str (metrics.accuracy_score(y_train, y_tr)*100) + \"%\")\n",
    "acc=metrics.accuracy_score(y_test, y_pred)\n",
    "wprec=metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "wrecall=metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "wf1=metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Test Accuracy is \"  + str (acc*100) + \"%\")\n",
    "print(\"Weighted Precision Score is \"  + str (wprec*100) + \"%\")\n",
    "print(\"Weighted Recall Score is \"  + str (wrecall*100) + \"%\")\n",
    "print(\"Weighted F1 Score is \"  + str (wf1*100) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Confusion Matrix \"+\"\\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification Report\"+\"\\n\",classification_report(y_test, y_pred))\n",
    "# weighted_precisions.append([\"Support Vector Machine Model 1\", wprec, acc, wrecall, wf1])   #appending models score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "### Tabular Comparison of all 8 implemented models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*126)\n",
    "text = \"|{:<34}|{:^14}|{:^24}|{:^24}|{:^24}|\"\n",
    "print(text.format(\"              Models\",\"Accuracy\", \"Weighted Precision\", \"Weighted Recall\", \"Weighted F1-score\"))\n",
    "print('-'*126)\n",
    "\n",
    "for i in range(len(weighted_precisions)):\n",
    "    print(text.format(\"  \"+weighted_precisions[i][0], round(float(weighted_precisions[i][2]),3), round(float(weighted_precisions[i][1]),3),round(float(weighted_precisions[i][3]),3),round(float(weighted_precisions[i][4]),3)))\n",
    "    \n",
    "print('-'*126)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {},
   "source": [
    "### Graphical Comparison of all 8 implemented models on the basis of Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy=[]\n",
    "models_names=[\"RF 1\", \"RF 2\", \"RF 3\", \"RF 4\", \"CB 1\",  \"CB 2\",  \"CB 3\",  \"CB 4\"]\n",
    "for i in weighted_precisions:\n",
    "    accuracy.append(float(i[2]))\n",
    "    \n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(models_names, accuracy)\n",
    "plt.xlabel('Model Names')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Comparison of Model Accuraccies')\n",
    "for i, score in enumerate(accuracy):\n",
    "    plt.text(i, score, str(round(float(score), 3)), ha='center', va='bottom')\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    ">It can be observed that the best Random Forest Model is RF2. Best Cat Boost Model is CB4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "### USER INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_variables=[1,2,3,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(user_values):\n",
    "    crimescore_prediction=rf_2.predict(user_values)\n",
    "#     print(\"Prediction using Random Forest : \", target_variables[crimescore_prediction[0]])\n",
    "    print(\"Prediction using Random Forest : \", crimescore_prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catBoost(user_values):\n",
    "    crimescore_prediction=cat_4.predict(user_values)\n",
    "#     print(\"Prediction using CatBoost : \", target_variables[crimescore_prediction[0][0]])\n",
    "    print(\"Prediction using Cat Boost : \", crimescore_prediction[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knnClassifier(user_values):\n",
    "    crimescore_prediction=knn_1.predict(user_values)\n",
    "#     print(\"Prediction using KNN Classifier : \", target_variables[crimescore_prediction[0]])\n",
    "    print(\"Prediction using KNN Classifier : \", crimescore_prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Welcome To Our Crime Score Prediction Program : \")\n",
    "print()\n",
    "print(\"Here you will answer some of our questions in numbers only :\")\n",
    "print()\n",
    "\n",
    "# Loop until a valid time of incident is entered\n",
    "while True:\n",
    "    try:\n",
    "        time = int(input(\"What is the Time of Crime Incident (12PM - 5PM or 5PM - 8PM or 8PM- 5AM or 5AM - 12PM):\\nFor 12PM - 5PM Press 0, For 5PM - 8PM Press 2, For 8PM- 5AM Press 3, For 5AM - 12PM Press 1 : \"))\n",
    "        if time not in [0, 1, 2, 3]:\n",
    "            raise ValueError(\"Invalid input! Please enter 0, 1, 2, or 3.\")\n",
    "        break  # Exit the loop if a valid time is entered\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print(\"Please try again.\")\n",
    "        print()\n",
    "\n",
    "# Loop until a valid latitude is entered\n",
    "while True:\n",
    "    try:\n",
    "        latitude = float(input(\"What is the latitude of the area of crime : \"))\n",
    "        if latitude < 24.75000 or latitude > 25.10000:\n",
    "            raise ValueError(\"Invalid latitude! Please enter a value between 24.75000 and 25.10000.\")\n",
    "        break  # Exit the loop if a valid latitude is entered\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print(\"Please try again.\")\n",
    "        print()\n",
    "\n",
    "# Loop until a valid longitude is entered\n",
    "while True:\n",
    "    try:\n",
    "        longitude = float(input(\"What is the longitude of the area of crime : \"))\n",
    "        if longitude < 66.80000 or longitude > 67.36444:\n",
    "            raise ValueError(\"Invalid longitude! Please enter a value between 66.80000 and 67.36444.\")\n",
    "        break  # Exit the loop if a valid longitude is entered\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print(\"Please try again.\")\n",
    "        print()\n",
    "\n",
    "values = np.array([time, latitude, longitude])\n",
    "\n",
    "# Reshape the data point to the expected input shape for the model\n",
    "v_features_array = values.reshape(1, -1)\n",
    "\n",
    "# Scaling (uncomment if needed)\n",
    "# t_values = scaler.transform(v_features_array)\n",
    "\n",
    "print()\n",
    "choose = input(\"Which Model you would like to test : \\n-->Press 1 for Random Forest\"+\n",
    "               \"\\n-->Press 2 for CatBoost\\n-->Press 3 for KNN Classifier\\n-->Press any other key for Exit\\nEnter your option : \")\n",
    "print()\n",
    "\n",
    "print(\"-\"*46)\n",
    "text = \"|{:^28}|{:^15}|\"\n",
    "print(text.format(\"Parameters\", \"Values\"))\n",
    "print(\"-\"*46)\n",
    "print(text.format(\"Time of Incident\", time))\n",
    "print(text.format(\"Latitude\", latitude))\n",
    "print(text.format(\"Longitude\", longitude))\n",
    "print(\"-\"*46)\n",
    "print()\n",
    "\n",
    "if choose == '1':\n",
    "    randomForest(v_features_array)\n",
    "    print(\"Thank you! For participating in our program...\")\n",
    "elif choose == '2':\n",
    "    catBoost(v_features_array)\n",
    "    print(\"Thank you! For participating in our program...\")\n",
    "elif choose == '3':\n",
    "    knnClassifier(v_features_array)\n",
    "    print(\"Thank you! For participating in our program...\")    \n",
    "else:\n",
    "    print(\"Thank you! For participating in our program...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
