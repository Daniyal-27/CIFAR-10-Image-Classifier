# -*- coding: utf-8 -*-
"""CV_Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kJnhNsxbT0uteSJrMDzXLVRb8E3P4PRw

# **CV CEA**

####**In the code below, we have used Keras to build an image classification model trained on the CIFAR-10 dataset. It uses the following layers/functions:**
*   For building the Model - CNN, Maxpooling and Dense Layers.

*   For Activation Function - ReLU (in CNN layers for handling image pixels) and Softmax (for final classification).

*   For handling Overfitting (Regularizing) - DropOut Layer.

*   For normalizing/standardizing the inputs between the layers (within the network) and hence accelerating the training, providing regularization and reducing the generalization error - Batch Normalization Layer.
"""

# Importing required libraries
import keras
from keras.datasets import cifar10
from keras.models import Sequential
from keras import datasets, layers, models
from keras.utils import to_categorical
from keras import regularizers
from keras.layers import Dense, Dropout, BatchNormalization
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping

"""### **Exploring Dataset**"""

ju# Loading dataset & splitting it into train and test set
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# Checking the shape of train and test set
print(train_images.shape)
print(train_labels.shape)
print(test_images.shape)
print(test_labels.shape)

# Checking the number of unique classes in dataset
print(np.unique(train_labels))
print(np.unique(test_labels))

# Creating a list of all the class labels
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

# Visualizing some of the images from the training dataset
plt.figure(figsize=[10,10])
for i in range (35):    # for first 35 images
  plt.subplot(7, 7, i+1)
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
  plt.imshow(train_images[i], cmap=plt.cm.binary)
  plt.xlabel(class_names[train_labels[i][0]])
plt.show()

"""### **Data Preprocessing**


*   We will apply Standardization/Normalization to convert all pixel values to values between 0 and 1.
*   We will convert image type to float since we are applying one hot encoding which needs the data to be of type float by default.
*   The reason for using to_categorical (one hot encoding) is that the loss function that we are using in this code (categorical_crossentropy) when compiling the model needs data to be one hot encoded.




"""

# Standardizing (255 is the total number of pixels an image can have)
train_images = train_images / 255.0
test_images = test_images / 255.0

# Converting the pixels data to float type
train_images = train_images.astype('float32')
test_images = test_images.astype('float32')

# One hot encoding the target class (labels)
num_classes = 10
train_labels = to_categorical(train_labels, num_classes)
test_labels = to_categorical(test_labels, num_classes)

"""### **Building a Machine Learning (CNN) Model**"""

# Creating a sequential model and adding layers to it

model = Sequential()

model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32,32,3)))
model.add(layers.BatchNormalization())
model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(layers.Dropout(0.3))

model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(layers.Dropout(0.5))

model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(layers.Dropout(0.5))

model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(num_classes, activation='softmax'))    # num_classes = 10

# Checking the model summary
model.summary()

"""### **Configuring and Compiling Model**


*   Optimizer used during Back Propagation for weight and bias adjustment - Adam (adjusts the learning rate adaptively).
*   Loss Function used - Categorical Crossentropy (used when multiple categories/classes are present).
*   Metrics used for evaluation - Accuracy.





"""

model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])

"""### **Training and Evaluating Model**"""

# Define the EarlyStopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Fit the model with EarlyStopping callback
history = model.fit(train_images, train_labels, batch_size=64, epochs=40, validation_data=(test_images, test_labels), callbacks=[early_stopping])

# CODE EXPLANATION
# EarlyStopping Callback: Monitors the validation loss (val_loss) and stops training if it does not improve for 5 consecutive epochs (patience=5).
# It also restores the best weights obtained during training.
# callbacks Parameter: Includes the early_stopping callback in the fit method, ensuring that training will stop early if the monitored quantity does not improve.
# This approach helps in preventing overfitting and ensures that the model training is efficient.

"""
#### ==> **Evaluation Visualization**
*   Using Loss Curve - Comparing the Training Loss with the Testing Loss over increasing Epochs.
*   Using Accuracy Curve - Comparing the Training Accuracy with the Testing Accuracy over increasing Epochs.
"""

# Plotting Loss Curve
plt.figure(figsize=[6,4])
plt.plot(history.history['loss'], 'black', linewidth=2.0)
plt.plot(history.history['val_loss'], 'green', linewidth=2.0)
plt.legend(['Training Loss', 'Validation Loss'], fontsize=14)
plt.xlabel('Epochs', fontsize=10)
plt.ylabel('Loss', fontsize=10)
plt.title('Loss Curves', fontsize=12)

# Plotting Accuracy Curve
plt.figure(figsize=[6,4])
plt.plot(history.history['accuracy'], 'black', linewidth=2.0)
plt.plot(history.history['val_accuracy'], 'blue', linewidth=2.0)
plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14)
plt.xlabel('Epochs', fontsize=10)
plt.ylabel('Accuracy', fontsize=10)
plt.title('Accuracy Curves', fontsize=12)

"""### **Predicting Results**"""

# Making the Predictions
pred = model.predict(test_images)
print(pred)

# Converting predictions into label index
pred_classes = np.argmax(pred, axis=1)
print(pred_classes)

# Plotting "Actual Vs Predicted" Results

fig, axes = plt.subplots(5, 5, figsize=(15,15))
axes = axes.ravel()

for i in np.arange(0, 25):
    axes[i].imshow(test_images[i])
    axes[i].set_title("True: %s \nPredict: %s" % (class_names[np.argmax(test_labels[i])], class_names[pred_classes[i]]))
    axes[i].axis('off')
    plt.subplots_adjust(wspace=1)